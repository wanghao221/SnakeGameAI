# 鱼皮编程学习路线

> 励志打造最贴心的编程学习路线

- [Java 学习路线](./docs/roadmap/Java学习路线.md)
- [前端学习路线](./docs/roadmap/前端学习路线.md)
- [Linux 学习路线](./docs/roadmap/Linux学习路线.md)
- [Python 学习路线](./docs/roadmap/Python学习路线.md)
- [大厂研发流程](./docs/roadmap/大厂研发流程.md)


# SnakeGameAI
使用强化学习和深度 Q 学习的 AI 驱动的蛇游戏。

# 贪吃蛇游戏人工智能
使用强化学习和深度 Q 学习的 AI 驱动的蛇游戏。

Snake 游戏实际上有一个微不足道的、无与伦比的解决方案。它可以通过简单的非 ML 技术通过遍历板的每个块来驱动，这提供了无与伦比的解决方案，但它非常耗时且非常暴力。

但我们将使用强化学习技术。

## 强化学习
第一个问题是为什么我们使用强化学习而不是监督机器学习，答案是，在监督机器学习算法中需要使用输入和称为目标的“正确答案”进行训练。在这个例子中，我们不不知道在游戏的每个阶段采取的最佳行动是什么，所以传统的方法是行不通的。
在强化学习中，我们有两个主要组成部分：环境（我们的游戏）和代理（我们的 Snake ......或者更准确地说，是驱动我们 Snake 动作的深度神经网络）。每次智能体执行一个动作时，环境都会给智能体一个奖励，奖励可以是正面的，也可以是负面的，这取决于该特定状态下动作的好坏程度。

深度强化学习 (DRL) 将 RL 的上述思想与深度神经网络相结合。神经网络学习“Q 函数”，它将当前环境状态作为输入，并输出包含每个可能动作的预期奖励的向量。然后代理可以选择最大化 Q 函数的动作。基于这个动作，游戏然后将环境更新为新状态并分配奖励（例如，吃苹果+10，撞墙-10）。在训练开始时，Q 函数只是由一个随机初始化的神经网络逼近。
我将一步一步解释这个SnakeAI的实现。

使用 pygame 模块设计了一个由用户控制的简单蛇棋盘游戏：https://juejin.cn/post/6989431581806952455
